const fs = require('fs');
const path = require('path');
const { promisify } = require('util');
const { Worker, isMainThread, parentPort, workerData } = require('worker_threads');

const readFileAsync = promisify(fs.readFile);
const writeFileAsync = promisify(fs.writeFile);

const dbFilePath = path.join(__dirname, 'database.json');
const batchInterval = 1000; // 1 second for batch processing
const cache = new Map();
let batchQueue = new Map();
let isProcessingBatch = false;

/**
 * Load the database from JSON file asynchronously.
 * @returns {Promise<Object>} The database object.
 */
const loadDatabase = async () => {
    try {
        const data = await readFileAsync(dbFilePath, 'utf-8');
        const jsonData = JSON.parse(data);
        Object.keys(jsonData).forEach(key => {
            cache.set(key, jsonData[key]);
        });
        return jsonData;
    } catch (error) {
        return {};
    }
};

/**
 * Save the database to JSON file asynchronously using streams.
 * @param {Object} data - The database object to save.
 * @returns {Promise<void>}
 */
const saveDatabase = async (data) => {
    const writeStream = fs.createWriteStream(dbFilePath);
    writeStream.write(JSON.stringify(data, null, 2));
    writeStream.end();
    return new Promise((resolve, reject) => {
        writeStream.on('finish', resolve);
        writeStream.on('error', reject);
    });
};

/**
 * Resolve conflicts between existing and incoming records using advanced strategies.
 * @param {Object} existing - The existing record in the database.
 * @param {Object} incoming - The incoming record to merge.
 * @returns {Object} The merged record.
 */
const resolveConflicts = (existing, incoming) => {
    return { ...existing, ...incoming, version: existing.version + 1 };
};

/**
 * Initialize in-memory cache from the database.
 * @returns {Promise<void>}
 */
const initializeCache = async () => {
    const data = await loadDatabase();
    for (const id in data) {
        cache.set(id, data[id]);
    }
};

/**
 * Process the batch queue to merge records.
 * @param {Map} batchQueue - The batch queue with records to process.
 * @returns {Map} The batched data with merged records.
 */
const processBatchQueue = (batchQueue) => {
    const batchedData = new Map();
    for (const [id, record] of batchQueue) {
        const existingRecord = cache.get(id) || { version: 0 };
        const mergedRecord = resolveConflicts(existingRecord, record);
        batchedData.set(id, mergedRecord);
    }
    return batchedData;
};

/**
 * Process the batch queue periodically to update the JSON file asynchronously using a worker thread.
 * @returns {Promise<void>}
 */
const processBatchQueueMain = async () => {
    if (batchQueue.size > 0 && !isProcessingBatch) {
        isProcessingBatch = true;

        const worker = new Worker(__filename, {
            workerData: { batchQueue: Array.from(batchQueue) },
        });

        worker.on('message', async (batchedData) => {
            for (const [id, record] of batchedData) {
                cache.set(id, record);
            }
            const database = Object.fromEntries(cache);
            await saveDatabase(database);
            isProcessingBatch = false;
        });

        worker.on('error', (error) => {
            console.error('Worker error:', error);
            isProcessingBatch = false;
        });

        batchQueue.clear();
    }
    setTimeout(processBatchQueueMain, batchInterval);
};

if (isMainThread) {
    // Main thread logic
    (async () => {
        await initializeCache();
        processBatchQueueMain();
    })();

    /**
     * Save a record with conflict resolution and batching.
     * @param {string} id - The unique identifier for the record.
     * @param {Object} newRecord - The new record to save.
     */
    const saveWith2030DataResolution = (id, newRecord) => {
        newRecord.version = newRecord.version || 1;
        batchQueue.set(id, newRecord);
    };

    // Export the functions for public use
    module.exports = {
        loadDatabase,
        saveWith2030DataResolution
    };

} else {
    // Worker thread logic
    const batchedData = processBatchQueue(new Map(workerData.batchQueue));
    parentPort.postMessage(Array.from(batchedData));
}
